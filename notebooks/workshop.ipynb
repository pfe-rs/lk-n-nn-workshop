{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vežbe iz neuralnih mreža\n",
    "## Seminar Primenjene Fizike i  Elektronike 2022\n",
    "\n",
    "Dobrodošli na vežbe iz neuralnih mreža :) Na ovim vežbama ćemo kroz seriju zadataka iterativno graditi koncepte potrebne za uspešno korišćenje i treniranje neuralnih mreža. Vežbe su konceptualno podeljene u sledeće celine:\n",
    "\n",
    "* Uvod: Funkcija gubitka i gradijentni spust\n",
    "* Neuron\n",
    "    * Zaključivanje neuronom\n",
    "    * Aktivacione funkcije\n",
    "    * Komputacioni graf i propagacija greške unazad\n",
    "* Neuralne mreže i slojevi\n",
    "    * Slojevi\n",
    "        * Softmax sloj\n",
    "        * Gusto povezani sloj\n",
    "    * Povezivanje slojeva\n",
    "* Treniranje neuralne mreže\n",
    "* Testiranje neuralne mreže\n",
    "\n",
    "\n",
    "Par reči ohrabrenja pre nego što krenemo:\n",
    "\n",
    "* Vežbe se oslanjaju na matematičke koncepte koji su varovatno mnogima od vas strani :( Ovo nije urađeno zato što autor vežbi želi da vas muči, već zato što nije znao kako da ih zaobiđe. Ukoliko neka formula nije u potpunosti jasna, nije strašno. Cilj ovih vežbi je da izađete sa jakom intuicijom zašto i kako određene stvari rade, za šta nije nužno potrebno savršeno razumevanje kompletne matematike u pozadini. Upravo zbog toga će svaka formula biti posebno diskutovana, i njena svrha rečima objašnjena.\n",
    "* Vežbe se dodatno oslanjaju na određeno programersko znanje, ali ni u jednom zadatku neće biti potrebno da implementirate više od 5 - 6 linija koda. Kod koji se u zadacima traži se mahom tiče `numpy` biblioteke za matematička izračunavanja, i najčešće će se u osnovi svoditi na množenje matrica i slične operacije.\n",
    "* __Nemojte se plašiti da pitate šta god vam nije jasno, autor vežbi u srednjoj školi ništa od ovoga ne bi znao da reši :)__\n",
    "\n",
    "Konačno par tehničkih stavki:\n",
    "\n",
    "* Od vas će se tražiti da popunite samo određene delove koda koji će jasno biti obeleženi `TODO: Opis zadatka` komentarima u okviru koda. Ukoliko se eksplicitno ne zahteva promena određene linije koda, molimo vas da ih ne dirate.\n",
    "* Uprkos tome da kod treba da se menja samo na određenim mestima, definisanim stavkom iznad, __svaka ćelija sa kodom treba da se izvrši__. Izvršavanje ćelije se može izvršiti `shift-ENTER` kombinacijom koja izvršava trenutno obeleženu ćeliju i prelazi na sledeću. Ćelije sa tekstom se takođe mogu izvršavati, ali to utiče samo na izgled teksta u okviru ćelije.\n",
    "* Iako se većina koda koji je od interesa se nalazi u okviru ove sveske, postoje određeni delovi koda koji se nalaze u okviru `trainer.py` fajla. Autor vas poziva da prođete i kroz taj fajl ako vas zanima, ali vam njegov sadržaj nije od interesa sve do samog kraja vežbi.\n",
    "\n",
    "Krećemo :)\n",
    "\n",
    "Za početak izvršite ćeliju ispod kucanjem `shift-ENTER` skraćenice. Ćelija učitava potrebne biblioteke za ostatak radionice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uvod: Funkcija gubitka i gradijentni spust\n",
    "\n",
    "Cilj obučavanja modela u supervizijskom (nadgledanom) pristupu u mašinskom učenju je minimizacija greške između zadatih vrednosti, u nastavku vežbi obeleženih sa $y$, i vrednosti koje supervizijski model predviđa, $\\hat{y}$. Greška modela se može izraziti na različite načine:\n",
    "\n",
    "* $(\\hat{y} - y)^2$: kvadratna greška\n",
    "* $|\\hat{y} - y|$: apsolutna greška\n",
    "* $y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})$: `log` greška\n",
    "\n",
    "Greška modela je sinonimna sa terminom __funkcije gubitka__ $L(y, \\hat{y})$. Drugim rečima, cilj obučavanja u supervizijskom pristupu je minimizacija funkcije gubitka $L(y, \\hat{y})$, koja direktno zavisi od ponuđene ispravne vrednosti $y$ i predviđene vrednosti $\\hat{y}$. Budući da funkcija gubitka zavisi od izlaza modela $\\hat{y}$, ona posledično zavisi i od svih parametara od kojih sam izlaz zavisi, odnosno od njegovih težina $w$. Zbog toga se funkcija gubitka može zapisati i u obliku $L(w)$.\n",
    "\n",
    "Iako postoje različiti metodi kojima se može postići minimalna greška, u ovoj vežbi nam je od interesa algoritam __gradijentnog spusta__. Cilj algoritma gradijentnog spusta je pronalaženje onih parametara $w$ modela, za koje je funkcija gubitka $L(w)$ minimialna. Algoritam gradijentnog spusta iterativno prati \"nagib\" funkcije, na taj način dolazeći do minimuma, kao što je prikazano na sledećoj slici.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/loss_function.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "\"Nagib\" funkcije se može opisati prvim izvodom funkcije po oređenoj promenjivoj, što ćemo u ovim vežbama obeležavati kao $\\frac{\\partial L(w)}{\\partial w}$. Značenje ovog izraza je sledeće. Zanima nas nagib funkcije $L(w)$ (brojilac), po parametru $w$ (imenilac). Simbol $\\partial$ specifično obeležava __parcijalni izvod__ funkcije, što znači da funkcija potencijalno može zavisiti od različitih promenjivih. Dvodimenzionalna funkcija $L(w, v)$ tako može imati parcijalne izvode po promenjivama $w$ i $v$, koji se obeležavaju sa $\\frac{\\partial L(w, v)}{\\partial w}$, i $\\frac{\\partial L(w, v)}{\\partial v}$ respektivno.\n",
    "\n",
    "Algoritam __gradijentnog spusta__ je sledeći:\n",
    "\n",
    "1. Inicijalizovati nasumične vrednosti $w$\n",
    "2. Odabrati stopu učenja $\\alpha$\n",
    "3. Do konvergencije (dok god se značajno menja $L(w)$) raditi:\n",
    "    1. Izračunati parcijalni izvod po parametru $w$: $\\frac{\\partial L(w)}{\\partial w}$\n",
    "    2. Osvežiti trenutnu vrednost parametra $w$ sledećim izrazom:\n",
    "        * $w \\leftarrow w - \\alpha \\cdot \\frac{\\partial L(w)}{\\partial w}$\n",
    "\n",
    "Na slici iznad plave strelice pokazuju u kom smeru bi pokazivao izvod funkcije po promenjivoj $w$.\n",
    "\n",
    "### Primer\n",
    "\n",
    "Na slici je data funkcija $L(w) = (w - 2)^2 -1$, čiji je izvod $\\frac{\\partial L(w)}{\\partial w} = 2(w-2)$. Na osnovu pseudo-koda datog iznad, popuniti probni algoritam gradijentnog spusta dat ispod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = 5\n",
    "learning_rate = 0.1\n",
    "L_w = (w - 2)**2 - 1\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    # TODO: Popuniti čemu treba da budu jednake promenjive dw i w, na osnovu pseudo-koda algoritma gradijentnog spusta.  \n",
    "    \n",
    "    L_w = (w - 2)**2 - 1\n",
    "    print(f'Trenutna vrednost parametra w je {w:.3f}. Funkcija gubitka za zadatu vrednost iznosi {L_w:.3f}.')\n",
    "print(f'Okvirna vrednost parametra w za koji se postiže minimum funkcije je {w:.3f}. Funkcija gubitka postiže minimalnu vrednost od {L_w:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron\n",
    "\n",
    "### Zaključivanje neuronom\n",
    "\n",
    "Neuralne mreže su izgrađene od neurona. Slika neurona je data ispod. Neuron sa slike prima vektor $\\textbf{x} = [x_0, x_1, x_2]^\\top$, i ima asocirane __težine__ (__parametre__) $\\textbf{w} = [w_0, w_1, w_2]^\\top$.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/neuron.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "Neuron radi sledeće izračunavanje:\n",
    "\n",
    "$$ z = \\sum_{i=1} x_iw_i = \\textbf{w}^\\top\\textbf{x} \\\\ \\hat{y} = \\sigma(z)$$\n",
    "\n",
    "$z$ se dobira kao skalarni proizvod vektora $\\textbf{x}$ i $\\textbf{w}$, na koji se primenjuje nelinearna __aktivaciona funkcija__ $\\sigma$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivacione funkcije\n",
    "\n",
    "Aktivacione funkcije daju neuralnim mrežama mogućnost da predstavljaju proizvoljnu funkciju, odnosno proizvoljno preslikavanje. Drugim rečima, neuralne mreže ne bi bili univerzalni aproksimatori kada aktivacione funkcije ne bi postojale.\n",
    "\n",
    "U nastavku su date tri aktivacione funkcije:\n",
    "\n",
    "1. IdentityActivation\n",
    "2. SigmoidActivation\n",
    "3. TanhActivation\n",
    "\n",
    "Zadatak je da se dovrše metode `forward(cls, preactivation)` i `backward(cls, activation, output_grad)`. Metoda `forward(cls, preactivation)` implementira samo preslikavanje aktivacione funkcije $\\sigma(z)$, gde je $z$ označeno kao `preactivation`. Metoda `backward(cls, activation, output_grad)` implementira parcijalni izvod aktivacione funkcije $\\sigma(z)$ po promenjivoj $z$, $\\frac{\\partial}{\\partial z} \\sigma(z)$. Ovde argument metode `activation` predstavlja vrednost koja se dobija primenom aktiacione funkcije, $\\text{activation} = \\sigma(z)$, dok će argument `output_grad` biti objašnjen kasnije i __nije ga potrebno koristiti u kodu koji je potrebno da dovršite__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationType(Enum):\n",
    "    \"\"\" Defines available activation functions. \"\"\"\n",
    "    IDENTITY = 1\n",
    "    TANH = 2\n",
    "    SIGMOID = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `identity` aktivaciona funkcija\n",
    "\n",
    "Za zagrevanje je data aktivaciona funkcija identiteta, koja samo radi $\\sigma(z) = z$. Ovakva aktivaciona funkcija se nikada ne koristi, budući da ništa i ne radi, ali prolazak kroz nju je koristan zarad razumevanja ostalih.\n",
    "\n",
    "##### Zadatak\n",
    "\n",
    "Parcijalni izvod aktivacione funkcije identiteta po $z$ je: $\\frac{\\partial}{\\partial z} \\sigma(z) = \\frac{\\partial z}{\\partial z} = 1$. __Znajući samu aktivacionu funkciju, i njen parcijalni izvod po $z$, dovršiti metode `forward(cls, preactivation)` i `backward(cls, activation, output_grad)` ispod__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IdentityActivation:\n",
    "    \"\"\" Class that implements identity activation function (y = x). \"\"\"\n",
    "    @classmethod\n",
    "    def forward(cls, preactivation):\n",
    "        \"\"\" Implements forward pass. Since identity just returns preactivation as is (y = x). \"\"\"\n",
    "        return_value = None\n",
    "        # TODO: Replace dummy implementation below with identity forward pass.\n",
    "        return return_value\n",
    "\n",
    "    @classmethod\n",
    "    def backward(cls, activation, output_grad):\n",
    "        \"\"\" Implements backward pass. Since identity activation derivative is array of 1s (y = x => dy/dx = 1). \"\"\"\n",
    "        activation_derivative = None\n",
    "        # TODO: Replace dummy implementation below with identity backward pass.\n",
    "        return np.multiply(output_grad, activation_derivative)\n",
    "\n",
    "assert((IdentityActivation.forward(np.ones((1, 2)) * 0.5) == np.ones((1, 2)) * 0.5).all())\n",
    "assert((IdentityActivation.backward(np.ones((1, 2)) * 0.5, np.ones((1, 2)) * 0.5) == np.ones((1, 2)) * 0.5).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sigmoid` aktivaciona funkcija\n",
    "\n",
    "Sigmoidna aktivaciona funkcija se do pre par godina vrlo često koristila, ali je u skorije vreme zamenjena funkcijama sa boljim karakteristikama. Logistička regresija, međutim, i dalje koristi sigmoidnu aktivacionu funkciju jer skalira izlaz na opseg $[0, 1]$, te se njen izlaz može interpretirati kao verovatnoća. Na slici ispod je punom linijom obeležena sigmoidna funkcija, dok je isprekidanom linijom dat njen izvod po $z$.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/sigmoid.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "$$\\frac{\\partial y}{\\partial z} = \\sigma(z)(1-\\sigma(z))$$\n",
    "\n",
    "Razlog zašto se sigmoidna funkcija ređe koristi danas je zbog problema __nestajućih gradijenata__. Da li neko sa slike vidi zašto? Ukoliko ne vidite, biće nešto jasnije kada sveska dođe do celine koja se bavi propagacijom greške unazad.\n",
    "\n",
    "#### Zadatak\n",
    "\n",
    "Parcijalni izvod aktivacione sigmoide po $z$ je dat iznad, i iznosi: $\\frac{\\partial}{\\partial z} \\sigma(z) = \\sigma(z)(1-\\sigma(z))$. Drugim rečima, iznosi čemu god je jednak izlaz te funkcije pomnožen sa (1 - čemu god je jednak izlaz te funkcije). __Znajući samu aktivacionu funkciju, i njen parcijalni izvod po $z$, dovršiti metode `forward(cls, preactivation)` i `backward(cls, activation, output_grad)` ispod__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation:\n",
    "    \"\"\" Class that implements sigmoid activation function (y = e^x / (1 + e^x). \"\"\"\n",
    "    @classmethod\n",
    "    def forward(cls, preactivation):\n",
    "        return_value = None\n",
    "        \"\"\" Implements forward pass. Apply sigmoid to preactivation and return it. \"\"\"\n",
    "        # TODO: Replace dummy implementation below with sigmoid forward pass.\n",
    "        return return_value\n",
    "\n",
    "    @classmethod\n",
    "    def backward(cls, activation, output_grad):\n",
    "        \"\"\" Implements backward pass (y = sigmoid(x) => dy/dx = sigmoid(x) * (1 - sigmoid(x)) = y * (1 - y)). \"\"\"\n",
    "        activation_derivative = None\n",
    "        # TODO: Replace dummy implementation below with sigmoid backward pass.\n",
    "        return np.multiply(output_grad, activation_derivative)\n",
    "\n",
    "assert((SigmoidActivation.forward(np.zeros((1, 2))) == np.ones((1, 2)) * 0.5).all())\n",
    "assert((SigmoidActivation.backward(np.ones((1, 2)) * 0.5, np.ones((1, 2))) == np.ones((1, 2)) * 0.25).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tanh` aktivaciona funkcija\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/tanh.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "$$\\sigma(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "$$\\frac{\\partial \\sigma(z)}{\\partial z} = 1 - \\tanh^2(z)$$\n",
    "\n",
    "\n",
    "#### Zadatak\n",
    "\n",
    "Parcijalni izvod hiperbolićkog tangensa po $z$ je dat iznad, i iznosi: $\\frac{\\partial \\sigma(z)}{\\partial z} = 1 - \\tanh^2(z)$. Drugim rečima, iznosi 1 -  kvadrirana vrednost čemu god je jednak izlaz te funkcije. __Znajući samu aktivacionu funkciju, i njen parcijalni izvod po $z$, dovršiti metode `forward(cls, preactivation)` i `backward(cls, activation, output_grad)` ispod__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhActivation:\n",
    "    \"\"\" Class that implements tanh activation function (y = tanh(x)). \"\"\"\n",
    "    @classmethod\n",
    "    def forward(cls, preactivation):\n",
    "        \"\"\" Implements forward pass. Apply tanh to preactivation and return it (y = tanh(x)). \"\"\"\n",
    "        return_value = None\n",
    "        # TODO: Replace dummy implementation below with tanh forward pass.\n",
    "        return return_value\n",
    "\n",
    "    @classmethod\n",
    "    def backward(cls, activation, output_grad):\n",
    "        \"\"\" Implements backward pass (y = tanh(x) => dy/dx = (1 - tanh^2(x)). \"\"\"\n",
    "        activation_derivative = None\n",
    "        # TODO: Replace dummy implementation below with tanh backward pass.\n",
    "        return np.multiply(output_grad, activation_derivative)\n",
    "\n",
    "assert((TanhActivation.forward(np.zeros((1, 2))) == np.zeros((1, 2))).all())\n",
    "assert((TanhActivation.backward(np.zeros((1, 2)), np.ones((1, 2))) == np.ones((1, 2))).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kod u ćeliji ispod samo bira u odnosu na `activation_type` koju će aktivacionu funkciju vratiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_activation(activation_type):\n",
    "    \"\"\" Activation factory function. Based on the give type creates and returns corresponding activation.\n",
    "        :param activation_type: Type of activation.\n",
    "    \"\"\"\n",
    "    activation = None\n",
    "    if activation_type == ActivationType.IDENTITY:\n",
    "        activation = IdentityActivation()\n",
    "    elif activation_type == ActivationType.TANH:\n",
    "        activation = TanhActivation()\n",
    "    elif activation_type == ActivationType.SIGMOID:\n",
    "        activation = SigmoidActivation()\n",
    "    else:\n",
    "        raise Exception(\"Unknown activation type.\")\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komputacioni graf i propagacija greške unazad\n",
    "\n",
    "#### Komputacioni graf\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/neuron.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "Za neuron, i posledično za neuralne mreže, vezuje se koncept __komputacionog grafa__. Komputacioni graf prati sva izračunavanja koje neuron, ili neuralna mreža, vrši od ulaza do izlaza. Slika neurona je već data, ali je ponovljena i iznad. __Komputacioni graf__ koji se vezuje za neuron je dat na slici ispod. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/neuron_computation_graph.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "Svako izračunavanje je uokvireno elipsom ili kvadratom, u zavisnosti od toga da li se radi o dobijanju preaktivacione $z$ vrednosti, ili aktivacione vrednosti $\\sigma(z)$. Ulaz u komputacioni graf čine sve promenjive koje model koristi, odnosno sam ulaz u model $\\textbf{x}$, težine modela $\\textbf{w}$, i pomeraj modela $b$. U slučaju neurona ulaz i težine su vektori, dok je pomeraj skalar. Konačno, na samom kraju komputacionog grafa se nalazi __funkcija gubitka__ $L(y, \\hat{y})$ koja direktno evaluira kvalitet modela --- koliko su njegove predikcije $\\hat{y}$ bliske zazatim izlazima $y$.\n",
    "\n",
    "#### Propagacija greške unazad\n",
    "\n",
    "__Propagacija greške unazad__ je poslednji, i najkompleksniji, fundamentalni koncept koji je potreban za potpuno razumevanje neuralnih mreža. Ovo poglavlje će sadržati najviše matematike koja je nekima od vas možda strana, ali to ne znači da se ne može shvatiti. Štaviše, većinu koncepata za razumevanje smo već objasnili kada smo govorili o __grešci modela__, odnosno __funkciji gubitka__, njenom __parcijalnom izvodu__, i __gradijentnom spustu__.\n",
    "\n",
    "##### Izvod složene funkcije\n",
    "\n",
    "Kao što je već rečeno, kod učenja sa supervizijom želimo da predviđanje modela $\\hat{y}$ bude što bliže zadatim vrednostima $y$. Blizinu merimo greškom modela, odnosno fukcijom gubitka $L(y, \\hat{y})$. Sa druge strane, učenje modela, neurona, neuralne mreže ili nečeg trećeg, se svodi na učenje optimalnih __parametara__, odnosno  __težina__ modela $\\textbf{w}$ (i pomeraja $b$). __Cilj učenja je da propagira grešku $L(y, \\hat{y})$ do parametara modela $\\textbf{w}$ i $b$.__\n",
    "\n",
    "Posmatrano iz ugla gradijentnog spusta, želimo u svakoj iteraciji videti za koliko da osvežimo parametre $\\textbf{w}$. Kao što je već prikazano u uvodnom primeru, ova veličina se može dobiti kao parcijalni izvod funkcije gubitka po promenjivoj $\\textbf{w}$, $\\frac{\\partial}{\\partial \\textbf{w}}L(y, \\hat{y})$.\n",
    "\n",
    "__Ovde dolazimo do srži problema.__ Naime, znamo čemu je jednak izvod funkcije sa jednim parametrom po tom parametru, međutim ukoliko vrednost tog parametra zavisi od neke pređašnje funkcije, onda se mora koristiti pravilo __izvoda složene funkcije__! Takav je slučaj kod nas --- funkcija gubtka $L(y, \\hat{y})$ direktno zavisi od $\\hat{y}$, ali $\\hat{y}$ dalje zavisi od $z$, dok najposle $z$ zavisi od $\\textbf{w}$. Kako bismo izračunali $\\frac{\\partial}{\\partial \\textbf{w}}L(y, \\hat{y})$, potrebno je da uračunamo sve ove zavisnosti.\n",
    "\n",
    "##### Propagacija greške unazad\n",
    "\n",
    "Na slici ispod je data jednačina čemu iznose $\\frac{\\partial}{\\partial \\textbf{w}}L(y, \\hat{y})$ i $\\frac{\\partial}{\\partial b}L(y, \\hat{y})$ u crvenoj boji. Do njih se dolazi primenom pravila __izvoda složene funkcije__, i primenom algoritma __propagacije greške unazad__.\n",
    "\n",
    "__Primetite najpre da se jednačine vrlo lako mogu dobiti__ ako se krene od kraja komputacionog grafa, $L(y, \\hat{y})$, i redom primenjuju percijalni izvodi __isključivo po parametrima od kojih svaki element direktno zavisi.__ Ove vrednosti su obeležene __ljubičastom bojom__. Naime, najpre na samom kraju izračunamo $\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}}$. Zatim u sledećem koraku, vidimo da $\\hat{y}$ zavisi od $z$, pa izračunamo $\\frac{\\partial \\hat{y}}{\\partial z}$. Na kraju, napokon, vidimo da $z$ zavisi od $\\textbf{w}$ i $b$, pa zato i izračunamo $\\frac{\\partial z}{\\partial \\textbf{w}}$ i $\\frac{\\partial z}{\\partial b}$. __Pravilo izvoda složene funkcije__ nalaže da se konačni izvod $\\frac{\\partial}{\\partial \\textbf{w}}L(y, \\hat{y})$ dobija kao proizvod dobijenih parcijalnih izvoda.\n",
    "\n",
    "__Propagacija greške unazad__ se odnosi na to da svaki element grafa izračunavanja prosleđuje svoj parcijalni izvod unazad, prethodnom elementu. Tako poslednji element, $\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}}$, ne prima ništa jer je poslednji i samo prosleđuje $\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}}$ unazad. Element pre njega prima $\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}}$, izračunava svoj parcijalni izvod, i __prosleđuje njihov proizvod__ $\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial L(y, \\hat{y})}{\\partial \\hat{y}}$ unazad. Poslednji element će, na samom kraju, proslediti kompletnu jednačinu parametrima $\\textbf{w}$ i $b$.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/neuron_computation_derivatives.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "#### Pitanja\n",
    "\n",
    "1. Da li neko vidi čemu je služio argument funkcije `output_grad` u `backward(cls, activation, output_grad)` kod implementacija aktivacionih funkcija u ranijoj vežbi?\n",
    "2. Da li smo mogli da propagiramo gradijent ka $\\textbf{x}$. Ako jesmo, čemu bi on bio jednak? Takođe, šta bi to imalo za posledicu?\n",
    "3. U ovom poglavlju je objašnjivan jedan neuron. Da li neko vidi kako se ovi koncepti primenjuju na neuralnu mrežu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuralne mreže i slojevi\n",
    "\n",
    "Slaganjem pojedinačnih neurona u __slojeve__, i slaganjem datih slojeva dobija se neuralna mreža. Na slici ispod je prikazana neuralna mreža sa __jednim skrivenim slojem__ od 4 neurona, i izlaznim slojem od jednog neurona.\n",
    "\n",
    "Ovde vas podstičem da se vratite na sliku neurona i uočite razlike:\n",
    "* U odnosu na pre, budući da sada imamo više neurona po sloju, je preaktivacija $\\textbf{z}^{[1]}$ vektor, a ne skalar.\n",
    "* Posledično, težine prvog sloja više nisu vektor $\\textbf{w}$, već matrica $\\textbf{W}^{[1]}$. Obratite pažnju na dimenziju matrice.\n",
    "* Imamo još jedan sloj, međutim budući da se on sastoji od samo jednog neurona, isti je kao neuron sa ranije slike.\n",
    "\n",
    "Prvi sloj mreže na slici ispod se naziva __gusto povezani sloj__, budući da se svaki element njegovog ulaza (u ovom slučaju $\\textbf{x}$)) spaja sa svakim elementom (neuronom) datog sloja.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/nn.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "Komputacioni graf je dat na slici ispod. Boje jednačina imaju isto značenje kao i ranije, pri čemu sada samih jednačina ima više zbog dubljeg komputacionog grafa.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"resources/nn_computation_graph.png\" alt=\"drawing\" width=\"50%\"/>\n",
    "</p>\n",
    "\n",
    "#### Pitanja\n",
    "\n",
    "1. Zašto je $\\textbf{b}^{[1]}$ sada vektor?\n",
    "2. U kontekstu neuralne mreže, koja potencijalno može imati jako puno naslaganih slojeva (daleko više od dva!), da li neko vidi zbog čega postoji problem __nestajućeg gradijenta__ ako bismo koristili sigmoidnu aktivacionu funkciju? Na koji način hiperbolički tangens to popravlja?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slojevi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax sloj\n",
    "\n",
    "$\\textbf{y}_i = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithCrossEntropyLayer:\n",
    "    \"\"\" Class that implements softmax + cross-entropy functionality. \"\"\"\n",
    "    def __init__(self, inputs_count):\n",
    "        \"\"\"\n",
    "        Constructor, creates internal objects.\n",
    "        :param inputs_count: number of inputs (== number of outputs).\n",
    "        \"\"\"\n",
    "        # Outputs of the layer.\n",
    "        self.y_hat = np.zeros((inputs_count, 1), dtype=float)\n",
    "        # Gradients with respect to inputs.\n",
    "        self.a_gradients = np.zeros((inputs_count, 1), dtype=float)\n",
    "        # The most probable class (index of max output).\n",
    "        self.y_max = None\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        \"\"\"\n",
    "        Performs forward pass on this layer.\n",
    "        :param x_input: Input array for this layer.\n",
    "        \"\"\"\n",
    "        # Calculate output as softmax of inputs.\n",
    "        # TODO: Implement softmax below as [y_hat] = softmax([x_input]).\n",
    "        exp = np.exp(x_input)\n",
    "        assert not np.isinf(exp).any()\n",
    "        norm = sum(exp)\n",
    "        self.y_hat = exp / norm\n",
    "        # Save the most probable class.\n",
    "        self.y_max = np.argmax(self.y_hat)\n",
    "\n",
    "    def backward(self, target):\n",
    "        \"\"\"\n",
    "        Performs backward pass on this layer.\n",
    "        :param target: Expected output (to be used in loss function).\n",
    "        \"\"\"\n",
    "        self.a_gradients = np.copy(self.y_hat)\n",
    "        self.a_gradients[target, 0] -= 1\n",
    "\n",
    "    def prediction(self):\n",
    "        \"\"\" Returns the most probably class for the most recent forward call. \"\"\"\n",
    "        return self.y_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gusto povezani sloj\n",
    "\n",
    "#### Zadatak 1: Prolazak unapred kroz sloj\n",
    "\n",
    "Gledajući formulu __gusto povezanog sloja__  sa komputacionog grafa, dovršiti `forward(self, x_input)` metodu. \n",
    "\n",
    "#### Zadatak 2: Propagacije greške unazad\n",
    "\n",
    "Za rešavanje ovog zadatka je potrebno gledati komputacioni graf sa poslednje slike. Specifično, najkorisnije je gledati drugu i treću ćeliju: $z^{[2]} = \\textbf{W}^{{[2]}\\top}  \\textbf{a}^{[1]} + b^{[1]}$ i $\\sigma(z^{[2]})$. Ove dve ćelije se posmatraju zajedno jer sloj podrazumeva i izračunavanje preaktivacije $z^{[2]}$, i aktivacije $\\sigma(z^{[2]})$.\n",
    "\n",
    "Od vas se traži da dovršite metodu `backward(self, output_grad)`. Kako biste to postigli, potrebno je najpre:\n",
    "* Uočiti čemu je na slici jednaka promenjiva `output_grad`. Ovo nije neophodno za samu implementaciju, ali jeste za razumevanje celine.\n",
    "* Uočiti izvode po kojim promenjivama sa slike je sloj zadužen da izračuna? Kojom bojom su oni obeleženi?\n",
    "* Proizvod kojih izvoda sloj treba u algoritmu propagacije greške unazad da propagira unazad? \n",
    "\n",
    "Imajući u vidu da je:\n",
    "\n",
    "* $\\frac{\\partial \\hat{y}}{\\partial z^{[2]}}$ zavisi od tipa aktivacione funkcije koja se koristi, i da ste to već implementirali. Ova promenjiva u kodu se naziva `z_gradients`\n",
    "* $\\frac{\\partial z^{[2]}}{\\partial \\textbf{W}^{[2]}} = \\frac{\\partial \\hat{y}}{\\partial z^{[2]}} \\cdot \\textbf{x}^\\top$. Ova promenjiva u kodu se naziva `w_gradients`\n",
    "* $\\frac{\\partial z^{[2]}}{\\partial b^{[2]}} = \\frac{\\partial \\hat{y}}{\\partial z^{[2]}}$. Ova promenjiva u kodu se naziva `b_gradients`\n",
    "* $\\frac{\\partial z^{[2]}}{\\partial \\textbf{a}^{[1]}} = \\textbf{W}^{[2]\\top} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{[2]}}$. Ova promenjiva u kodu se naziva `a_gradients`\n",
    "\n",
    "dovršite naznačene delove u metodi `backward(self, output_grad)`.\n",
    "\n",
    "#### Zadatak 3: Osvežavanje težina algoritmom gradijentnog spusta\n",
    "\n",
    "Dovršiti osvežavanje težina u metodi `update_weights(self, alpha)` na osnovu algoritma gradijentnog spusta, prezentovanog u prvom poglavlju."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    \"\"\" Class that implements fully connected layer functionality. \"\"\"\n",
    "    def __init__(self, rng, inputs_count, outputs_count, activation_type):\n",
    "        \"\"\"\n",
    "        Constructor, creates internal objects.\n",
    "        :param rng: A random number generator used to initialize weights.\n",
    "        :param inputs_count: Dimensionality of input.\n",
    "        :param outputs_count: Dimensionality of output.\n",
    "        \"\"\"\n",
    "        # Create weights array and initialize it randomly.\n",
    "        self.w = np.asarray(\n",
    "            rng.uniform(\n",
    "                low=-np.sqrt(6. / (inputs_count + outputs_count)),\n",
    "                high=np.sqrt(6. / (inputs_count + outputs_count)),\n",
    "                size=(outputs_count, inputs_count)\n",
    "            ),\n",
    "            dtype=float\n",
    "        )\n",
    "        # Create biases and zero them out.\n",
    "        self.b = np.zeros((outputs_count, 1), dtype=float)\n",
    "        # Allocate all gradient arrays.\n",
    "        self.w_gradients = np.zeros((outputs_count, inputs_count), dtype=float)\n",
    "        self.b_gradients = np.zeros((outputs_count, 1), dtype=float)\n",
    "        self.a_gradients = np.zeros((inputs_count, 1), dtype=float)\n",
    "        # Set activation function according to given type.\n",
    "        self.sigma = create_activation(activation_type)\n",
    "        # Declare input/output arrays to be used later.\n",
    "        self.x = None\n",
    "        self.a = np.zeros((outputs_count, 1), dtype=float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass for this layer using formula [output] = activation([input] * [wights] + [biases])\n",
    "        :param x_input: Input array for forward pass.\n",
    "        \"\"\"\n",
    "        # TODO: Implement fully connected layer forward compute below as\n",
    "        # [activation] = activation([w] * [x] + [b]).\n",
    "        # First calculate preactivation as [z] = [w] * [x] + [b]\n",
    "        # Now apply activation function on preactivation to obtain output ([self.a] = activation([z])).\n",
    "\n",
    "        \n",
    "        # Remember input to be able to compute gradients with respect to weights.\n",
    "        self.x = x\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        \"\"\"\n",
    "        Performs backward pass for this layer.\n",
    "        :param output_grad: Gradients of the outputs.\n",
    "        \"\"\"\n",
    "        # Calculate preactivation gradients.\n",
    "        z_gradients = self.sigma.backward(self.a, output_grad)\n",
    "        # Based on z_gradients calculate bias, weights and input gradients.\n",
    "        # TODO: Implement fully connected layer backward. Compute b_gradients, w_gradients and a_gradients\n",
    "        #       using the formulas below:\n",
    "        #       [b_gradients] = [z_gradients]\n",
    "        #       [w_gradients] = [z_gradients] * [input]T\n",
    "        #       [a_gradients] = [w]T * [z_gradients]\n",
    "\n",
    "    def update_weights(self, alpha):\n",
    "        \"\"\"\n",
    "        Updates weights for this layer using given learning rate and already computed gradients.\n",
    "        :param alpha: Learning rate to be used.\n",
    "        \"\"\"\n",
    "        # TODO: Implement weight and bias update steps from gradient descent algorithm from the first chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Povezivanje slojeva\n",
    "\n",
    "Došlo je vreme da napravimo prvu neuralnu mrežu :) Neuralna mreža ispod se sastoji od tri sloja:\n",
    "\n",
    "1. Skriveni sloj sa aktivacionom funkcijom `tanh` (ovo slobodno u kodu promenite na `sigmoid` ako vam je draže)\n",
    "2. Skriveni sloj sa aktivacionom funkcijom identiteta\n",
    "3. Izlazni `softmax` sloj budući da se danas bavimo zadatkom __klasifikacije na n kategorija__\n",
    "\n",
    "#### Zadatak 1: Prolazak unapred kroz mrežu\n",
    "\n",
    "Implementirati metodu `forward(self, x)`. Imati u vidu da smo već implementirali sve slojeve koje mreža koristi, te je dovoljno da se adekvatno iskoriste njihove `forward(self, x)` metode :) Konačno, potrebno je videti na koji način dohvatiti izlaze svakog sloja.\n",
    "\n",
    "#### Zadatak 2: Propagacija greške unazad\n",
    "\n",
    "Implementirati metodu `backward(self, y)` koja prima zadatu vrednost $y$ i propagira $L(y, \\hat{y})$ unazad. Imati u vodu da su sve `backward` funkcije već implementirane, i da je samo potrebno pozvati ih u ispravnom redosledu sa ispravnim podacima. Konačno, potrebno je videti na koji način dohvatiti gradijente izlaza svakog sloja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron Class\n",
    "    A multilayer perceptron is a feedforward artificial neural network model that has one hidden fully connected layer,\n",
    "    and one output fully connected layer, both with nonlinear activations. The top layer (third one) is a softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, rng, inputs_count, hidden_count, outputs_count):\n",
    "        \"\"\"Initialize the parameters for the multilayer perceptron.\n",
    "        :param rng: A random number generator used to initialize weights\n",
    "        :param inputs_count: Number of input units, the dimension of the space in which the datapoints lie.\n",
    "        :param hidden_count: number of hidden units.\n",
    "        :param outputs_count: Number of output units, the dimension of the space in which the labels lie.\n",
    "        \"\"\"\n",
    "        # Create hidden layer (first fully connected layer).\n",
    "        self.hidden_layer = FullyConnectedLayer(\n",
    "            rng=rng,\n",
    "            inputs_count=inputs_count,\n",
    "            outputs_count=hidden_count,\n",
    "            activation_type=ActivationType.TANH\n",
    "        )\n",
    "\n",
    "        # Create output layer (second fully connected layer).\n",
    "        self.output_layer = FullyConnectedLayer(\n",
    "            rng=rng,\n",
    "            inputs_count=hidden_count,\n",
    "            outputs_count=outputs_count,\n",
    "            activation_type=ActivationType.IDENTITY\n",
    "        )\n",
    "\n",
    "        # Create softmax layer.\n",
    "        self.softmax_layer = SoftmaxWithCrossEntropyLayer(inputs_count=outputs_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Performs forward pass through the network by sequentially invoking forward on child layers.\n",
    "        :param x_input: Input to the network.\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass. Remember that outputs of each layer are stored in .a attributes.\n",
    "\n",
    "    def backward(self, y):\n",
    "        \"\"\" Performs backward pass through the network by sequentially invoking backward on child layers in reverse\n",
    "        order.\n",
    "        :param t_target: Expected output of the network.\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass. Remember that during backprop computation graph is read from right to left.\n",
    "\n",
    "    def update_weights(self, alpha):\n",
    "        \"\"\" Performs update of the network weights by updating weight on each child layer.\n",
    "        :param alpha: Learning rate to be used for weight update.\n",
    "        \"\"\"\n",
    "        self.output_layer.update_weights(alpha)\n",
    "        self.hidden_layer.update_weights(alpha)\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.forward(x.reshape(x.shape[0], 1))\n",
    "        y_hat = self.softmax_layer.prediction()\n",
    "        return y_hat\n",
    "\n",
    "    def test(self, dataset):\n",
    "        \"\"\" Performs testing of the trained network on the given dataset. Returns accuracy of the network.\n",
    "        :param dataset: Dataset to be used for testing.\n",
    "        \"\"\"\n",
    "        x_input, y_target = dataset\n",
    "        error_count = 0\n",
    "        for i in range(x_input.shape[0]):\n",
    "            self.forward(x_input[i].reshape(x_input.shape[1], 1))\n",
    "            if self.softmax_layer.prediction() != y_target[i]:\n",
    "                error_count += 1\n",
    "        return float(error_count) / x_input.shape[0]\n",
    "\n",
    "neural_net = MLPNetwork(np.random, inputs_count=28 * 28, hidden_count=100, outputs_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treniranje neuralne mreže"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from trainer import train_nn_with_sgd, get_random_sample, sample_to_image, load_data\n",
    "\n",
    "dataset_path_os_normalized = os.path.join(\".\",\"data\",\"mnist.pkl\")\n",
    "_, _, test_set = load_data(dataset_path=dataset_path_os_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_random_sample(dataset=test_set)\n",
    "image = sample_to_image(x=x)\n",
    "print(f\"A typical example for class {y} looks like this.\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "ALPHA = 0.01\n",
    "\n",
    "train_nn_with_sgd(neural_net=neural_net, dataset_path=dataset_path_os_normalized, epochs_count=EPOCHS, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaključivanje neuralnom mrežom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = neural_net.inference(x=x)\n",
    "\n",
    "print(f\"Neural network prediction for class {y} and image below is {y_hat}\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testiranje neuralne mreže"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import test_nn\n",
    "\n",
    "dataset_path_os_normalized = os.path.join(\".\",\"data\",\"mnist.pkl\")\n",
    "model_path_os_normalized = os.path.join(\".\",\"model\",\"nn.pkl\")\n",
    "test_nn(model_path_os_normalized, dataset_path_os_normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea0c376a256e2d4f1d520d02610d07fa9317181a0f87e08d36ab97c29d79bdd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
